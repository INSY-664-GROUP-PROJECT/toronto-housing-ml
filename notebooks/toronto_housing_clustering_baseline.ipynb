{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3804ea77",
      "metadata": {},
      "source": [
        "# Phase 1 Restart: EDA + Preprocessing + Spatial Grid Foundation\n",
        "\n",
        "This notebook redoes Phase 1 with the requested scope:\n",
        "- strong EDA + preprocessing for all four datasets,\n",
        "- **Step 1:** project latitude/longitude to a metric CRS,\n",
        "- **Step 2:** create a **sparse spatial base grid** (occupied cells only).\n",
        "\n",
        "Clustering is intentionally excluded in this notebook and will be handled in the next phase.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca802306",
      "metadata": {},
      "source": [
        "## Scope and Method\n",
        "\n",
        "We treat this as a multi-layer spatial foundation problem:\n",
        "1. clean each source with explicit logging,\n",
        "2. inspect quality and distributional behavior,\n",
        "3. convert all points to a projected system for distance-safe operations,\n",
        "4. build a sparse grid where each occupied cell is a micro-geographic unit for future feature aggregation and clustering.\n",
        "\n",
        "This notebook ends at grid creation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7d9be8",
      "metadata": {},
      "source": [
        "## Explanation Pattern\n",
        "\n",
        "Each major section follows:\n",
        "- **Question**\n",
        "- **Method**\n",
        "- **What we found**\n",
        "- **Why this matters for next-phase clustering**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f10a9722",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa429fdd",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyproj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92fb2a2",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:07.313631Z",
          "iopub.status.busy": "2026-02-24T22:58:07.313438Z",
          "iopub.status.idle": "2026-02-24T22:58:08.354444Z",
          "shell.execute_reply": "2026-02-24T22:58:08.353944Z"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import re\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from pyproj import Transformer\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.width', 180)\n",
        "sns.set_theme(style='whitegrid', context='notebook')\n",
        "\n",
        "NOTEBOOK_START = time.time()\n",
        "\n",
        "DATA_DIR = Path('../data')\n",
        "OUTPUT_DIR = Path('../outputs/eda')\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "TORONTO_BBOX = {\n",
        "    'lat_min': 43.55,\n",
        "    'lat_max': 43.90,\n",
        "    'lon_min': -79.65,\n",
        "    'lon_max': -79.10,\n",
        "}\n",
        "\n",
        "CELL_SIZE_M = 500\n",
        "MIN_RENTALS_PER_CELL = 5\n",
        "PROJECTED_CRS = 'EPSG:32617'  # UTM zone 17N (Toronto area)\n",
        "GEODETIC_CRS = 'EPSG:4326'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f71635a8",
      "metadata": {},
      "source": [
        "## Data Ingestion + Raw Audit\n",
        "\n",
        "**Question:** What is the baseline quality profile before preprocessing?\n",
        "\n",
        "**Method:** Load all datasets and compute rows, columns, duplicate rows, missingness, coordinate validity, and Toronto bbox coverage.\n",
        "\n",
        "**What we found:** The audit table and missingness charts expose where cleaning effort is needed.\n",
        "\n",
        "**Why this matters for next-phase clustering:** Reliable clustering starts with reliable inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4a5c2c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:08.356129Z",
          "iopub.status.busy": "2026-02-24T22:58:08.355988Z",
          "iopub.status.idle": "2026-02-24T22:58:08.927888Z",
          "shell.execute_reply": "2026-02-24T22:58:08.927513Z"
        }
      },
      "outputs": [],
      "source": [
        "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out.columns = [str(c).replace('ï»¿', '').strip() for c in out.columns]\n",
        "    return out\n",
        "\n",
        "\n",
        "def to_numeric(series: pd.Series) -> pd.Series:\n",
        "    return pd.to_numeric(series, errors='coerce')\n",
        "\n",
        "\n",
        "def valid_coord_mask(df: pd.DataFrame, lat_col: str, lon_col: str) -> pd.Series:\n",
        "    lat = to_numeric(df[lat_col])\n",
        "    lon = to_numeric(df[lon_col])\n",
        "    return lat.notna() & lon.notna() & lat.between(-90, 90) & lon.between(-180, 180)\n",
        "\n",
        "\n",
        "def bbox_mask(df: pd.DataFrame, lat_col: str, lon_col: str) -> pd.Series:\n",
        "    lat = to_numeric(df[lat_col])\n",
        "    lon = to_numeric(df[lon_col])\n",
        "    return lat.between(TORONTO_BBOX['lat_min'], TORONTO_BBOX['lat_max']) & lon.between(TORONTO_BBOX['lon_min'], TORONTO_BBOX['lon_max'])\n",
        "\n",
        "\n",
        "def parse_currency(value):\n",
        "    if pd.isna(value):\n",
        "        return np.nan\n",
        "    return pd.to_numeric(str(value).replace('$', '').replace(',', '').strip(), errors='coerce')\n",
        "\n",
        "\n",
        "def parse_bedroom_count(value):\n",
        "    if pd.isna(value):\n",
        "        return np.nan\n",
        "\n",
        "    text = str(value).strip().lower()\n",
        "    if text in {'studio', 'bachelor'}:\n",
        "        return 0.0\n",
        "\n",
        "    nums = re.findall(r'\\d+(?:\\.\\d+)?', text)\n",
        "    if not nums:\n",
        "        return np.nan\n",
        "    return float(nums[0])\n",
        "\n",
        "\n",
        "def normalize_rest_price(value):\n",
        "    mapping = {\n",
        "        'Under $10': 10.0,\n",
        "        '$11-30': 20.5,\n",
        "        '$31-60': 45.5,\n",
        "        'Above $61': 70.0,\n",
        "        'US$11-30': 20.5,\n",
        "    }\n",
        "    if pd.isna(value):\n",
        "        return np.nan\n",
        "    text = str(value).strip()\n",
        "    if text in mapping:\n",
        "        return mapping[text]\n",
        "\n",
        "    nums = [int(n) for n in re.findall(r'\\d+', text)]\n",
        "    if len(nums) >= 2:\n",
        "        return (nums[0] + nums[1]) / 2.0\n",
        "    if len(nums) == 1:\n",
        "        return float(nums[0])\n",
        "    return np.nan\n",
        "\n",
        "\n",
        "def winsorize_series(series: pd.Series, q_low=0.01, q_high=0.99):\n",
        "    low, high = series.quantile([q_low, q_high])\n",
        "    return series.clip(lower=low, upper=high), float(low), float(high)\n",
        "\n",
        "\n",
        "crime_raw = clean_columns(pd.read_csv(DATA_DIR / 'MCI_2014_to_2019.csv'))\n",
        "rent_2018_raw = clean_columns(pd.read_csv(DATA_DIR / 'Toronto_apartment_rentals_2018.csv'))\n",
        "rest_raw = clean_columns(pd.read_csv(DATA_DIR / 'trt_rest.csv'))\n",
        "subway_raw = clean_columns(pd.read_csv(DATA_DIR / 'toronto_subway_stations.csv'))\n",
        "\n",
        "rent_2018_raw = rent_2018_raw.copy()\n",
        "rent_2018_raw['rental_source'] = 'toronto_apartment_rentals_2018'\n",
        "\n",
        "rent_core_cols = ['Bedroom', 'Bathroom', 'Den', 'Address', 'Lat', 'Long', 'Price', 'rental_source']\n",
        "rent_base = rent_2018_raw.copy()\n",
        "for col in rent_core_cols:\n",
        "    if col not in rent_base.columns:\n",
        "        rent_base[col] = pd.NA\n",
        "rent_base = rent_base[rent_core_cols]\n",
        "\n",
        "rentfaster_path = DATA_DIR / 'rentfaster.csv'\n",
        "if rentfaster_path.exists():\n",
        "    rentfaster_raw = clean_columns(pd.read_csv(rentfaster_path))\n",
        "    rentfaster_norm = rentfaster_raw.copy()\n",
        "    rentfaster_norm['Bedroom'] = rentfaster_norm['beds'].apply(parse_bedroom_count)\n",
        "    rentfaster_norm['Bathroom'] = to_numeric(rentfaster_norm['baths'])\n",
        "    rentfaster_norm['Den'] = 0\n",
        "\n",
        "    street = rentfaster_norm['address'].astype('string').fillna('').str.strip()\n",
        "    city = rentfaster_norm['city'].astype('string').fillna('').str.strip()\n",
        "    province = rentfaster_norm['province'].astype('string').fillna('').str.strip()\n",
        "    rentfaster_norm['Address'] = (\n",
        "        street\n",
        "        + np.where(city.ne(''), ', ' + city, '')\n",
        "        + np.where(province.ne(''), ', ' + province, '')\n",
        "    ).str.strip(', ')\n",
        "\n",
        "    rentfaster_norm['Lat'] = to_numeric(rentfaster_norm['latitude'])\n",
        "    rentfaster_norm['Long'] = to_numeric(rentfaster_norm['longitude'])\n",
        "    rentfaster_norm['Price'] = rentfaster_norm['price']\n",
        "    rentfaster_norm['rental_source'] = 'rentfaster'\n",
        "\n",
        "    rentfaster_for_merge = rentfaster_norm[rent_core_cols]\n",
        "    rent_raw = pd.concat([rent_base, rentfaster_for_merge], ignore_index=True)\n",
        "else:\n",
        "    rent_raw = rent_base.copy()\n",
        "\n",
        "raw_datasets = {\n",
        "    'crime': crime_raw,\n",
        "    'rentals': rent_raw,\n",
        "    'restaurants': rest_raw,\n",
        "    'subway': subway_raw,\n",
        "}\n",
        "\n",
        "coord_cols = {\n",
        "    'crime': ('Lat', 'Long'),\n",
        "    'rentals': ('Lat', 'Long'),\n",
        "    'restaurants': ('Restaurant Latitude', 'Restaurant Longitude'),\n",
        "    'subway': ('latitude', 'longitude'),\n",
        "}\n",
        "\n",
        "audit_rows = []\n",
        "for name, df in raw_datasets.items():\n",
        "    lat_col, lon_col = coord_cols[name]\n",
        "    valid = valid_coord_mask(df, lat_col, lon_col)\n",
        "    bbox = bbox_mask(df, lat_col, lon_col)\n",
        "\n",
        "    audit_rows.append({\n",
        "        'dataset': name,\n",
        "        'rows': int(df.shape[0]),\n",
        "        'columns': int(df.shape[1]),\n",
        "        'duplicate_rows': int(df.duplicated().sum()),\n",
        "        'missingness_pct': round(float(df.isna().sum().sum() / (df.shape[0] * df.shape[1]) * 100), 4),\n",
        "        'coord_valid_pct': round(float(valid.mean() * 100), 4),\n",
        "        'toronto_bbox_coverage_pct': round(float(bbox.mean() * 100), 4),\n",
        "    })\n",
        "\n",
        "audit_df = pd.DataFrame(audit_rows).sort_values('dataset').reset_index(drop=True)\n",
        "audit_df.to_csv(OUTPUT_DIR / 'data_quality_audit.csv', index=False)\n",
        "audit_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ad6546",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:08.929179Z",
          "iopub.status.busy": "2026-02-24T22:58:08.929101Z",
          "iopub.status.idle": "2026-02-24T22:58:09.098647Z",
          "shell.execute_reply": "2026-02-24T22:58:09.098234Z"
        }
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for ax, (name, df) in zip(axes, raw_datasets.items()):\n",
        "    miss = (df.isna().mean() * 100).sort_values(ascending=False).head(8)\n",
        "    if miss.sum() == 0:\n",
        "        miss = pd.Series({'(no missing columns)': 0.0})\n",
        "    sns.barplot(x=miss.values, y=miss.index, ax=ax, color='#4C78A8')\n",
        "    ax.set_title(f'{name}: Top Missing Columns (%)')\n",
        "    ax.set_xlabel('Missing %')\n",
        "    ax.set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3b8a0c",
      "metadata": {},
      "source": [
        "## Preprocessing Engine\n",
        "\n",
        "**Question:** How do we apply deterministic preprocessing with traceability?\n",
        "\n",
        "**Method:** Use reusable filter/dedup helpers with step-level drop logging.\n",
        "\n",
        "**What we found:** Each transformation is accounted for in a drop log table.\n",
        "\n",
        "**Why this matters for next-phase clustering:** Reproducibility and auditability are critical for team confidence and model governance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c450cb4a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:09.099819Z",
          "iopub.status.busy": "2026-02-24T22:58:09.099731Z",
          "iopub.status.idle": "2026-02-24T22:58:09.102052Z",
          "shell.execute_reply": "2026-02-24T22:58:09.101719Z"
        }
      },
      "outputs": [],
      "source": [
        "drop_log = []\n",
        "\n",
        "\n",
        "def log_step(dataset: str, step: str, before: int, after: int, detail: str = ''):\n",
        "    dropped = int(before - after)\n",
        "    drop_log.append({\n",
        "        'dataset': dataset,\n",
        "        'step': step,\n",
        "        'rows_before': int(before),\n",
        "        'rows_after': int(after),\n",
        "        'dropped_rows': dropped,\n",
        "        'drop_pct': 0.0 if before == 0 else round(dropped / before * 100, 4),\n",
        "        'detail': detail,\n",
        "    })\n",
        "\n",
        "\n",
        "def apply_filter(df: pd.DataFrame, mask: pd.Series, dataset: str, step: str, detail: str = '') -> pd.DataFrame:\n",
        "    before = len(df)\n",
        "    out = df.loc[mask].copy()\n",
        "    log_step(dataset, step, before, len(out), detail)\n",
        "    return out\n",
        "\n",
        "\n",
        "def apply_dedup(df: pd.DataFrame, subset, dataset: str, step: str) -> pd.DataFrame:\n",
        "    before = len(df)\n",
        "    out = df.drop_duplicates(subset=subset).copy()\n",
        "    log_step(dataset, step, before, len(out), f'subset={subset}')\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dfe897a",
      "metadata": {},
      "source": [
        "## Preprocess Crime, Rentals, Restaurants, Subway\n",
        "\n",
        "**Question:** What cleaned point-level data should feed spatial grid construction?\n",
        "\n",
        "**Method:** Apply dataset-specific cleaning rules for temporal validity, duplicates, coordinates, plausibility limits, and pricing normalization.\n",
        "\n",
        "**What we found:** Final cleaned datasets are compact, consistent, and coordinate-safe.\n",
        "\n",
        "**Why this matters for next-phase clustering:** Better point quality means better spatial feature quality downstream.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3cbdc26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:09.103115Z",
          "iopub.status.busy": "2026-02-24T22:58:09.103033Z",
          "iopub.status.idle": "2026-02-24T22:58:09.435701Z",
          "shell.execute_reply": "2026-02-24T22:58:09.435323Z"
        }
      },
      "outputs": [],
      "source": [
        "# ----- Crime -----\n",
        "crime = crime_raw.copy()\n",
        "for col in crime.columns:\n",
        "    if pd.api.types.is_object_dtype(crime[col]) or pd.api.types.is_string_dtype(crime[col]):\n",
        "        crime[col] = crime[col].astype('string').str.strip()\n",
        "\n",
        "crime['occurrenceyear'] = to_numeric(crime['occurrenceyear'])\n",
        "crime['Lat'] = to_numeric(crime['Lat'])\n",
        "crime['Long'] = to_numeric(crime['Long'])\n",
        "crime['MCI'] = crime['MCI'].astype('string').str.strip()\n",
        "crime['event_unique_id'] = crime['event_unique_id'].astype('string').str.strip()\n",
        "crime['is_violent'] = crime['MCI'].str.lower().isin({'assault', 'robbery', 'homicide'}).astype(int)\n",
        "\n",
        "crime = apply_filter(crime, crime['occurrenceyear'].between(2014, 2019, inclusive='both'), 'crime', 'keep_occurrenceyear_2014_2019')\n",
        "crime = apply_filter(crime, crime[['Lat', 'Long', 'MCI']].notna().all(axis=1), 'crime', 'drop_missing_core_fields')\n",
        "crime = apply_filter(crime, bbox_mask(crime, 'Lat', 'Long'), 'crime', 'toronto_bbox_filter')\n",
        "crime = apply_dedup(crime, ['event_unique_id'], 'crime', 'dedup_event_unique_id')\n",
        "\n",
        "# ----- Rentals -----\n",
        "rent = rent_raw.copy()\n",
        "for col in rent.columns:\n",
        "    if pd.api.types.is_object_dtype(rent[col]) or pd.api.types.is_string_dtype(rent[col]):\n",
        "        rent[col] = rent[col].astype('string').str.strip()\n",
        "\n",
        "rent['Lat'] = to_numeric(rent['Lat'])\n",
        "rent['Long'] = to_numeric(rent['Long'])\n",
        "rent['Bedroom'] = to_numeric(rent['Bedroom'])\n",
        "rent['Bathroom'] = to_numeric(rent['Bathroom'])\n",
        "rent['Den'] = to_numeric(rent['Den'])\n",
        "rent['Price_num'] = rent['Price'].apply(parse_currency)\n",
        "\n",
        "rent = apply_dedup(rent, ['Address', 'Bedroom', 'Bathroom', 'Den', 'Lat', 'Long', 'Price'], 'rentals', 'dedup_listing_signature')\n",
        "rent = apply_filter(rent, rent['Address'].astype('string').str.contains('Toronto', case=False, na=False), 'rentals', 'keep_address_contains_toronto')\n",
        "rent = apply_filter(rent, rent[['Lat', 'Long', 'Bedroom', 'Bathroom', 'Price_num']].notna().all(axis=1), 'rentals', 'drop_missing_core_fields')\n",
        "rent = apply_filter(rent, bbox_mask(rent, 'Lat', 'Long'), 'rentals', 'toronto_bbox_filter')\n",
        "rent = apply_filter(rent, rent['Bedroom'].between(0, 6, inclusive='both'), 'rentals', 'bedroom_plausibility_0_6')\n",
        "rent = apply_filter(rent, rent['Bathroom'].between(0, 5, inclusive='both'), 'rentals', 'bathroom_plausibility_0_5')\n",
        "rent = apply_filter(rent, rent['Price_num'].between(400, 10000, inclusive='both'), 'rentals', 'price_plausibility_400_10000')\n",
        "\n",
        "rent['Price_clean'], q01, q99 = winsorize_series(rent['Price_num'], 0.01, 0.99)\n",
        "rent['price_was_winsorized'] = ((rent['Price_num'] < q01) | (rent['Price_num'] > q99)).astype(int)\n",
        "rent['price_per_bedroom'] = rent['Price_clean'] / rent['Bedroom'].clip(lower=1)\n",
        "log_step('rentals', 'winsorize_price_q01_q99', len(rent), len(rent), f'q01={q01:.2f};q99={q99:.2f};winsorized_rows={int(rent.price_was_winsorized.sum())}')\n",
        "\n",
        "# ----- Restaurants -----\n",
        "rest = rest_raw.copy()\n",
        "for col in rest.columns:\n",
        "    if pd.api.types.is_object_dtype(rest[col]) or pd.api.types.is_string_dtype(rest[col]):\n",
        "        rest[col] = rest[col].astype('string').str.strip()\n",
        "\n",
        "rest['Restaurant Latitude'] = to_numeric(rest['Restaurant Latitude'])\n",
        "rest['Restaurant Longitude'] = to_numeric(rest['Restaurant Longitude'])\n",
        "rest['Category'] = rest['Category'].astype('string').str.strip().replace({'<NA>': pd.NA}).fillna('Unknown')\n",
        "rest['Restaurant Name'] = rest['Restaurant Name'].astype('string').str.strip()\n",
        "rest['Restaurant Address'] = rest['Restaurant Address'].astype('string').str.strip()\n",
        "rest['restaurant_price_mid'] = rest['Restaurant Price Range'].apply(normalize_rest_price)\n",
        "rest['restaurant_price_missing_original'] = rest['restaurant_price_mid'].isna().astype(int)\n",
        "\n",
        "rest = apply_filter(rest, rest[['Restaurant Latitude', 'Restaurant Longitude']].notna().all(axis=1), 'restaurants', 'drop_missing_coords')\n",
        "rest = apply_filter(rest, bbox_mask(rest, 'Restaurant Latitude', 'Restaurant Longitude'), 'restaurants', 'toronto_bbox_filter')\n",
        "rest = apply_filter(rest, rest['Restaurant Address'].astype('string').str.contains('Toronto', case=False, na=False), 'restaurants', 'keep_address_contains_toronto')\n",
        "\n",
        "cat_median = rest.groupby('Category')['restaurant_price_mid'].transform('median')\n",
        "rest['restaurant_price_mid'] = rest['restaurant_price_mid'].fillna(cat_median)\n",
        "rest_global_med = float(rest['restaurant_price_mid'].median())\n",
        "rest['restaurant_price_mid'] = rest['restaurant_price_mid'].fillna(rest_global_med)\n",
        "log_step('restaurants', 'impute_restaurant_price_category_then_global', len(rest), len(rest), f'global_median={rest_global_med:.2f}')\n",
        "\n",
        "# venue-level for point density/grid occupancy\n",
        "rest_venue = apply_dedup(rest, ['Restaurant Name', 'Restaurant Address', 'Restaurant Latitude', 'Restaurant Longitude'], 'restaurants', 'dedup_restaurant_venue_signature')\n",
        "\n",
        "# ----- Subway -----\n",
        "subway = subway_raw.copy()\n",
        "for col in subway.columns:\n",
        "    if pd.api.types.is_object_dtype(subway[col]) or pd.api.types.is_string_dtype(subway[col]):\n",
        "        subway[col] = subway[col].astype('string').str.strip()\n",
        "\n",
        "subway['latitude'] = to_numeric(subway['latitude'])\n",
        "subway['longitude'] = to_numeric(subway['longitude'])\n",
        "subway = apply_filter(subway, subway[['latitude', 'longitude']].notna().all(axis=1), 'subway', 'drop_missing_coords')\n",
        "subway = apply_dedup(subway, ['name', 'latitude', 'longitude'], 'subway', 'dedup_station_signature')\n",
        "subway = apply_filter(subway, bbox_mask(subway, 'latitude', 'longitude'), 'subway', 'toronto_bbox_filter')\n",
        "\n",
        "cleaned_counts_df = pd.DataFrame([\n",
        "    {'dataset': 'crime', 'rows_clean': len(crime)},\n",
        "    {'dataset': 'rentals', 'rows_clean': len(rent)},\n",
        "    {'dataset': 'restaurants_venue', 'rows_clean': len(rest_venue)},\n",
        "    {'dataset': 'subway', 'rows_clean': len(subway)},\n",
        "])\n",
        "\n",
        "a = rent_raw['rental_source'].astype('string').fillna('unknown').value_counts().rename('rows_raw')\n",
        "b = rent['rental_source'].astype('string').fillna('unknown').value_counts().rename('rows_clean')\n",
        "rental_source_summary = (\n",
        "    pd.concat([a, b], axis=1)\n",
        "    .fillna(0)\n",
        "    .astype(int)\n",
        "    .reset_index()\n",
        "    .rename(columns={'index': 'rental_source'})\n",
        "    .sort_values('rows_clean', ascending=False)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "display(cleaned_counts_df)\n",
        "rental_source_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30878f76",
      "metadata": {},
      "source": [
        "## Preprocessing Log and Recap\n",
        "\n",
        "**Question:** How many rows were removed by each preprocessing rule?\n",
        "\n",
        "**Method:** Persist step-level drop log and summarize raw vs cleaned counts.\n",
        "\n",
        "**What we found:** Most reductions come from duplicate and location validity controls.\n",
        "\n",
        "**Why this matters for next-phase clustering:** Quantified preprocessing effects avoid hidden data shifts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd399b97",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:09.436979Z",
          "iopub.status.busy": "2026-02-24T22:58:09.436911Z",
          "iopub.status.idle": "2026-02-24T22:58:09.440372Z",
          "shell.execute_reply": "2026-02-24T22:58:09.440125Z"
        }
      },
      "outputs": [],
      "source": [
        "drop_log_df = pd.DataFrame(drop_log)\n",
        "drop_log_df.to_csv(OUTPUT_DIR / 'preprocessing_drop_log.csv', index=False)\n",
        "\n",
        "recap = pd.DataFrame([\n",
        "    {'dataset': 'crime', 'rows_raw': len(crime_raw), 'rows_clean': len(crime)},\n",
        "    {'dataset': 'rentals', 'rows_raw': len(rent_raw), 'rows_clean': len(rent)},\n",
        "    {'dataset': 'restaurants', 'rows_raw': len(rest_raw), 'rows_clean': len(rest_venue)},\n",
        "    {'dataset': 'subway', 'rows_raw': len(subway_raw), 'rows_clean': len(subway)},\n",
        "])\n",
        "\n",
        "recap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c111536",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:09.441434Z",
          "iopub.status.busy": "2026-02-24T22:58:09.441375Z",
          "iopub.status.idle": "2026-02-24T22:58:09.725031Z",
          "shell.execute_reply": "2026-02-24T22:58:09.724658Z"
        }
      },
      "outputs": [],
      "source": [
        "dup_summary = pd.DataFrame([\n",
        "    {\n",
        "        'dataset': name,\n",
        "        'duplicate_before': int(raw_datasets[name].duplicated().sum()),\n",
        "        'duplicate_after': int(df.duplicated().sum()),\n",
        "    }\n",
        "    for name, df in [('crime', crime), ('rentals', rent), ('restaurants', rest_venue), ('subway', subway)]\n",
        "])\n",
        "\n",
        "dup_long = dup_summary.melt(id_vars='dataset', var_name='stage', value_name='duplicate_rows')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=dup_long, x='dataset', y='duplicate_rows', hue='stage')\n",
        "plt.title('Duplicate Rows: Before vs After Preprocessing')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "dup_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c40d25cc",
      "metadata": {},
      "source": [
        "## EDA Highlights After Cleaning\n",
        "\n",
        "**Question:** What are the cleaned distributions that will influence future clustering?\n",
        "\n",
        "**Method:** Inspect temporal crime structure, rental price behavior, and amenity composition.\n",
        "\n",
        "**What we found:** Core distributional patterns are visible and cleaner than raw data.\n",
        "\n",
        "**Why this matters for next-phase clustering:** These are the feature-generating signals for later unsupervised learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57de3f2f",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:09.726157Z",
          "iopub.status.busy": "2026-02-24T22:58:09.726075Z",
          "iopub.status.idle": "2026-02-24T22:58:09.805798Z",
          "shell.execute_reply": "2026-02-24T22:58:09.805396Z"
        }
      },
      "outputs": [],
      "source": [
        "crime_year = (\n",
        "    crime.groupby('occurrenceyear', as_index=False)\n",
        "    .size()\n",
        "    .rename(columns={'size': 'incident_count'})\n",
        "    .sort_values('occurrenceyear')\n",
        ")\n",
        "\n",
        "top_mci = crime['MCI'].value_counts().head(12).rename_axis('MCI').reset_index(name='count')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "sns.lineplot(data=crime_year, x='occurrenceyear', y='incident_count', marker='o', ax=axes[0])\n",
        "axes[0].set_title('Crime Incidents by Year (Cleaned)')\n",
        "axes[0].set_xlabel('Year')\n",
        "axes[0].set_ylabel('Incident count')\n",
        "\n",
        "sns.barplot(data=top_mci, y='MCI', x='count', ax=axes[1], color='#54A24B')\n",
        "axes[1].set_title('Top Crime Categories (Cleaned)')\n",
        "axes[1].set_xlabel('Incident count')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff954830",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:09.806920Z",
          "iopub.status.busy": "2026-02-24T22:58:09.806850Z",
          "iopub.status.idle": "2026-02-24T22:58:09.981966Z",
          "shell.execute_reply": "2026-02-24T22:58:09.981621Z"
        }
      },
      "outputs": [],
      "source": [
        "rent_before = rent_raw.copy()\n",
        "rent_before['Price_num'] = rent_before['Price'].apply(parse_currency)\n",
        "rent_before = rent_before[rent_before['Price_num'].notna()]\n",
        "\n",
        "display_cap = np.nanpercentile(rent_before['Price_num'], 99.5)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "sns.histplot(rent_before['Price_num'], bins=50, ax=axes[0, 0], color='#B279A2')\n",
        "axes[0, 0].set_title('Rent Distribution (Raw Parsed)')\n",
        "axes[0, 0].set_xlim(0, display_cap)\n",
        "\n",
        "sns.histplot(rent['Price_clean'], bins=50, ax=axes[0, 1], color='#4C78A8')\n",
        "axes[0, 1].set_title('Rent Distribution (Cleaned + Winsorized)')\n",
        "axes[0, 1].set_xlim(0, display_cap)\n",
        "\n",
        "box_df = pd.DataFrame({\n",
        "    'raw': rent_before['Price_num'].clip(upper=display_cap),\n",
        "    'clean': rent['Price_clean'].clip(upper=display_cap),\n",
        "}).melt(var_name='stage', value_name='price')\n",
        "\n",
        "sns.boxplot(data=box_df, x='stage', y='price', hue='stage', legend=False, ax=axes[1, 0])\n",
        "axes[1, 0].set_title('Raw vs Clean Rent Boxplot')\n",
        "\n",
        "sns.violinplot(data=rent[rent['Bedroom'].between(0, 6, inclusive='both')], x='Bedroom', y='Price_clean', ax=axes[1, 1], color='#F58518', cut=0)\n",
        "axes[1, 1].set_title('Rent by Bedroom (Cleaned)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28dc6b9b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:09.983249Z",
          "iopub.status.busy": "2026-02-24T22:58:09.983163Z",
          "iopub.status.idle": "2026-02-24T22:58:10.068728Z",
          "shell.execute_reply": "2026-02-24T22:58:10.068393Z"
        }
      },
      "outputs": [],
      "source": [
        "price_bins = pd.cut(\n",
        "    rest_venue['restaurant_price_mid'],\n",
        "    bins=[0, 10.5, 30.5, 60.5, np.inf],\n",
        "    labels=['Under $10', '$11-30', '$31-60', 'Above $61'],\n",
        "    include_lowest=True,\n",
        ")\n",
        "\n",
        "price_counts = price_bins.value_counts().rename_axis('price_range').reset_index(name='count')\n",
        "top_categories = rest_venue['Category'].value_counts().head(15).rename_axis('Category').reset_index(name='count')\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "sns.barplot(data=price_counts, x='price_range', y='count', ax=axes[0], color='#E45756')\n",
        "axes[0].set_title('Restaurant Price Tiers (Cleaned Venue-Level)')\n",
        "axes[0].set_xlabel('Price tier')\n",
        "axes[0].set_ylabel('Venue count')\n",
        "\n",
        "sns.barplot(data=top_categories, y='Category', x='count', ax=axes[1], color='#72B7B2')\n",
        "axes[1].set_title('Top Restaurant Categories (Cleaned Venue-Level)')\n",
        "axes[1].set_xlabel('Venue count')\n",
        "axes[1].set_ylabel('')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82451443",
      "metadata": {},
      "source": [
        "## Step 1: Standardize Coordinate System (Projected CRS)\n",
        "\n",
        "**Question:** How do we convert all datasets into a distance-safe coordinate system?\n",
        "\n",
        "**Method:** Transform from WGS84 (`EPSG:4326`) to UTM Zone 17N (`EPSG:32617`) using `pyproj`.\n",
        "\n",
        "**What we found:** All cleaned points receive metric `x_m`/`y_m` coordinates.\n",
        "\n",
        "**Why this matters for next-phase clustering:** Grid construction and distance-based analytics require projected coordinates in meters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5e4a696",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:10.069903Z",
          "iopub.status.busy": "2026-02-24T22:58:10.069840Z",
          "iopub.status.idle": "2026-02-24T22:58:10.099725Z",
          "shell.execute_reply": "2026-02-24T22:58:10.099398Z"
        }
      },
      "outputs": [],
      "source": [
        "fwd_transformer = Transformer.from_crs(GEODETIC_CRS, PROJECTED_CRS, always_xy=True)\n",
        "inv_transformer = Transformer.from_crs(PROJECTED_CRS, GEODETIC_CRS, always_xy=True)\n",
        "\n",
        "\n",
        "def add_projected_xy(df: pd.DataFrame, lat_col: str, lon_col: str) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    lon = out[lon_col].to_numpy(dtype=float)\n",
        "    lat = out[lat_col].to_numpy(dtype=float)\n",
        "    x_m, y_m = fwd_transformer.transform(lon, lat)\n",
        "    out['x_m'] = x_m\n",
        "    out['y_m'] = y_m\n",
        "    return out\n",
        "\n",
        "\n",
        "crime_proj = add_projected_xy(crime, 'Lat', 'Long')\n",
        "rent_proj = add_projected_xy(rent, 'Lat', 'Long')\n",
        "rest_proj = add_projected_xy(rest_venue, 'Restaurant Latitude', 'Restaurant Longitude')\n",
        "subway_proj = add_projected_xy(subway, 'latitude', 'longitude')\n",
        "\n",
        "projected_summary = pd.DataFrame([\n",
        "    {\n",
        "        'dataset': 'crime',\n",
        "        'rows': len(crime_proj),\n",
        "        'x_min': float(crime_proj['x_m'].min()),\n",
        "        'x_max': float(crime_proj['x_m'].max()),\n",
        "        'y_min': float(crime_proj['y_m'].min()),\n",
        "        'y_max': float(crime_proj['y_m'].max()),\n",
        "    },\n",
        "    {\n",
        "        'dataset': 'rentals',\n",
        "        'rows': len(rent_proj),\n",
        "        'x_min': float(rent_proj['x_m'].min()),\n",
        "        'x_max': float(rent_proj['x_m'].max()),\n",
        "        'y_min': float(rent_proj['y_m'].min()),\n",
        "        'y_max': float(rent_proj['y_m'].max()),\n",
        "    },\n",
        "    {\n",
        "        'dataset': 'restaurants',\n",
        "        'rows': len(rest_proj),\n",
        "        'x_min': float(rest_proj['x_m'].min()),\n",
        "        'x_max': float(rest_proj['x_m'].max()),\n",
        "        'y_min': float(rest_proj['y_m'].min()),\n",
        "        'y_max': float(rest_proj['y_m'].max()),\n",
        "    },\n",
        "    {\n",
        "        'dataset': 'subway',\n",
        "        'rows': len(subway_proj),\n",
        "        'x_min': float(subway_proj['x_m'].min()),\n",
        "        'x_max': float(subway_proj['x_m'].max()),\n",
        "        'y_min': float(subway_proj['y_m'].min()),\n",
        "        'y_max': float(subway_proj['y_m'].max()),\n",
        "    },\n",
        "])\n",
        "\n",
        "projected_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75662e95",
      "metadata": {},
      "source": [
        "## Step 2: Create Adaptive Sparse Grid (Equal-Rental-Count Circles)\n",
        "\n",
        "**Question:** How do we build spatial units where low-density areas become larger and high-density areas become smaller without spatial overlap artifacts?\n",
        "\n",
        "**Method:**\n",
        "- Start from a very fine projected mesh (`100m x 100m`) and use occupied mesh cells as seed locations.\n",
        "- Grow each seed cell outward ring-by-ring, collecting nearby rentals until exactly **3 rentals** are available.\n",
        "- Stop growth immediately at 3 rentals and set the circle radius to the 3rd rental distance from the seed center.\n",
        "- Enforce one-to-one rental membership (each rental can belong to only one adaptive cell).\n",
        "- For crime/restaurants/subway counts, assign each point to its **nearest adaptive cell center** (no double counting across overlapping circles).\n",
        "\n",
        "**What we found:** Adaptive cells now reflect local density while avoiding inflated counts caused by overlapping circle coverage.\n",
        "\n",
        "**Why this matters for next-phase clustering:** Features become spatially stable and interpretable because each point contributes to exactly one adaptive unit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a5a046d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:10.101008Z",
          "iopub.status.busy": "2026-02-24T22:58:10.100944Z",
          "iopub.status.idle": "2026-02-24T22:58:10.503108Z",
          "shell.execute_reply": "2026-02-24T22:58:10.502825Z"
        }
      },
      "outputs": [],
      "source": [
        "K_RENTALS_PER_CELL = 3\n",
        "MESH_CELL_SIZE_M = 100.0\n",
        "MAX_GROWTH_RADIUS_M = 3000.0\n",
        "MAX_RING_STEPS = int(np.ceil(MAX_GROWTH_RADIUS_M / MESH_CELL_SIZE_M))\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "rent_core = rent_proj[['x_m', 'y_m', 'Lat', 'Long', 'Price_clean', 'price_per_bedroom']].reset_index(drop=True)\n",
        "rent_xy = rent_core[['x_m', 'y_m']].to_numpy(dtype=float)\n",
        "\n",
        "# Build a fine projected mesh and attach each rental to a mesh cell.\n",
        "mesh_origin_x = float(np.floor(rent_xy[:, 0].min() / MESH_CELL_SIZE_M) * MESH_CELL_SIZE_M)\n",
        "mesh_origin_y = float(np.floor(rent_xy[:, 1].min() / MESH_CELL_SIZE_M) * MESH_CELL_SIZE_M)\n",
        "grid_x = np.floor((rent_xy[:, 0] - mesh_origin_x) / MESH_CELL_SIZE_M).astype(int)\n",
        "grid_y = np.floor((rent_xy[:, 1] - mesh_origin_y) / MESH_CELL_SIZE_M).astype(int)\n",
        "\n",
        "rent_core['grid_x'] = grid_x\n",
        "rent_core['grid_y'] = grid_y\n",
        "\n",
        "cell_to_indices = defaultdict(set)\n",
        "for idx, (gx, gy) in enumerate(zip(grid_x, grid_y)):\n",
        "    cell_to_indices[(int(gx), int(gy))].add(int(idx))\n",
        "\n",
        "unassigned = set(range(len(rent_core)))\n",
        "blocked_seed_cells = set()\n",
        "adaptive_rows = []\n",
        "cell_id = 0\n",
        "\n",
        "\n",
        "def ring_cells(seed_cell, ring):\n",
        "    sx, sy = seed_cell\n",
        "    if ring == 0:\n",
        "        return [(sx, sy)]\n",
        "\n",
        "    out = []\n",
        "    for dx in range(-ring, ring + 1):\n",
        "        out.append((sx + dx, sy - ring))\n",
        "        out.append((sx + dx, sy + ring))\n",
        "    for dy in range(-ring + 1, ring):\n",
        "        out.append((sx - ring, sy + dy))\n",
        "        out.append((sx + ring, sy + dy))\n",
        "    return out\n",
        "\n",
        "\n",
        "def active_seed_cells(cell_index_map, blocked_cells):\n",
        "    ranked = []\n",
        "    for cell_key, members in cell_index_map.items():\n",
        "        if members and cell_key not in blocked_cells:\n",
        "            ranked.append((len(members), cell_key[1], cell_key[0], cell_key))\n",
        "\n",
        "    ranked.sort(key=lambda t: (-t[0], t[1], t[2]))\n",
        "    return [t[3] for t in ranked]\n",
        "\n",
        "\n",
        "while len(unassigned) >= K_RENTALS_PER_CELL:\n",
        "    seeds = active_seed_cells(cell_to_indices, blocked_seed_cells)\n",
        "    if not seeds:\n",
        "        break\n",
        "\n",
        "    seed_cell = seeds[0]\n",
        "    sx, sy = seed_cell\n",
        "    center_x = mesh_origin_x + (sx + 0.5) * MESH_CELL_SIZE_M\n",
        "    center_y = mesh_origin_y + (sy + 0.5) * MESH_CELL_SIZE_M\n",
        "\n",
        "    candidate_indices = set()\n",
        "    ring_used = None\n",
        "\n",
        "    # Grow from the seed mesh cell outward until we can form exactly K rentals.\n",
        "    for ring in range(MAX_RING_STEPS + 1):\n",
        "        for cell_key in ring_cells(seed_cell, ring):\n",
        "            if cell_key in cell_to_indices:\n",
        "                candidate_indices.update(cell_to_indices[cell_key])\n",
        "\n",
        "        if len(candidate_indices) >= K_RENTALS_PER_CELL:\n",
        "            ring_used = ring\n",
        "            break\n",
        "\n",
        "    # If this seed cannot find enough rentals within the growth cap, skip it.\n",
        "    if ring_used is None:\n",
        "        blocked_seed_cells.add(seed_cell)\n",
        "        continue\n",
        "\n",
        "    cand = np.array(sorted(candidate_indices), dtype=int)\n",
        "    d = np.sqrt(((rent_xy[cand] - np.array([center_x, center_y])) ** 2).sum(axis=1))\n",
        "    order = np.argsort(d)\n",
        "    selected = cand[order[:K_RENTALS_PER_CELL]]\n",
        "    kth_radius_m = float(d[order[K_RENTALS_PER_CELL - 1]])\n",
        "\n",
        "    if kth_radius_m > MAX_GROWTH_RADIUS_M:\n",
        "        blocked_seed_cells.add(seed_cell)\n",
        "        continue\n",
        "\n",
        "    radius_m = max(kth_radius_m, 1.0)\n",
        "\n",
        "    rsub = rent_core.iloc[selected]\n",
        "    adaptive_rows.append({\n",
        "        'adaptive_cell_id': f'ac_{cell_id:04d}',\n",
        "        'rental_count': int(len(selected)),\n",
        "        'seed_grid_x': int(sx),\n",
        "        'seed_grid_y': int(sy),\n",
        "        'growth_rings': int(ring_used),\n",
        "        'x_centroid': float(center_x),\n",
        "        'y_centroid': float(center_y),\n",
        "        'seed_local_k_radius_m': float(radius_m),\n",
        "        'radius_m': float(radius_m),\n",
        "        'mean_rent': float(rsub['Price_clean'].mean()),\n",
        "        'median_rent': float(rsub['Price_clean'].median()),\n",
        "        'mean_price_per_bedroom': float(rsub['price_per_bedroom'].mean()),\n",
        "        'member_rental_idx': ','.join(map(str, selected.tolist())),\n",
        "    })\n",
        "\n",
        "    # Enforce non-overlap in rental membership.\n",
        "    for ridx in selected:\n",
        "        ridx = int(ridx)\n",
        "        unassigned.discard(ridx)\n",
        "        ckey = (int(grid_x[ridx]), int(grid_y[ridx]))\n",
        "        if ckey in cell_to_indices and ridx in cell_to_indices[ckey]:\n",
        "            cell_to_indices[ckey].remove(ridx)\n",
        "            if len(cell_to_indices[ckey]) == 0:\n",
        "                del cell_to_indices[ckey]\n",
        "\n",
        "    blocked_seed_cells = {c for c in blocked_seed_cells if c in cell_to_indices}\n",
        "    cell_id += 1\n",
        "\n",
        "leftover_rental_indices = sorted(unassigned)\n",
        "leftover_rentals = rent_core.iloc[leftover_rental_indices].copy() if leftover_rental_indices else pd.DataFrame(columns=rent_core.columns)\n",
        "\n",
        "adaptive_grid = pd.DataFrame(adaptive_rows)\n",
        "\n",
        "# Convert adaptive cell centers back to lon/lat for mapping and downstream joins.\n",
        "c_lon, c_lat = inv_transformer.transform(adaptive_grid['x_centroid'].to_numpy(), adaptive_grid['y_centroid'].to_numpy())\n",
        "adaptive_grid['centroid_lon'] = c_lon\n",
        "adaptive_grid['centroid_lat'] = c_lat\n",
        "adaptive_grid['cell_area_km2'] = np.pi * (adaptive_grid['radius_m'] ** 2) / 1_000_000\n",
        "\n",
        "# Prepare projected point arrays for layer-level counts.\n",
        "crime_xy = crime_proj[['x_m', 'y_m']].to_numpy(dtype=float)\n",
        "rest_xy = rest_proj[['x_m', 'y_m']].to_numpy(dtype=float)\n",
        "subway_xy = subway_proj[['x_m', 'y_m']].to_numpy(dtype=float)\n",
        "crime_violent = crime_proj['is_violent'].to_numpy(dtype=int)\n",
        "\n",
        "centers_xy = adaptive_grid[['x_centroid', 'y_centroid']].to_numpy(dtype=float)\n",
        "\n",
        "\n",
        "def assign_to_nearest_center(points_xy, center_xy, chunk_size=20000):\n",
        "    if len(points_xy) == 0:\n",
        "        return np.array([], dtype=int), np.array([], dtype=float)\n",
        "\n",
        "    assigned_idx = np.empty(len(points_xy), dtype=int)\n",
        "    assigned_dist = np.empty(len(points_xy), dtype=float)\n",
        "\n",
        "    for start in range(0, len(points_xy), chunk_size):\n",
        "        end = min(start + chunk_size, len(points_xy))\n",
        "        block = points_xy[start:end]\n",
        "        d2 = ((block[:, None, :] - center_xy[None, :, :]) ** 2).sum(axis=2)\n",
        "        nn = np.argmin(d2, axis=1)\n",
        "        assigned_idx[start:end] = nn\n",
        "        assigned_dist[start:end] = np.sqrt(d2[np.arange(len(block)), nn])\n",
        "\n",
        "    return assigned_idx, assigned_dist\n",
        "\n",
        "\n",
        "# Non-overlap counting: each point is assigned to exactly one adaptive cell center.\n",
        "crime_assign_idx, crime_assign_dist = assign_to_nearest_center(crime_xy, centers_xy)\n",
        "rest_assign_idx, rest_assign_dist = assign_to_nearest_center(rest_xy, centers_xy)\n",
        "subway_assign_idx, subway_assign_dist = assign_to_nearest_center(subway_xy, centers_xy)\n",
        "\n",
        "crime_counts = np.bincount(crime_assign_idx, minlength=len(adaptive_grid)).astype(int)\n",
        "violent_counts = np.bincount(crime_assign_idx, weights=crime_violent.astype(float), minlength=len(adaptive_grid)).round().astype(int)\n",
        "restaurant_counts = np.bincount(rest_assign_idx, minlength=len(adaptive_grid)).astype(int)\n",
        "subway_counts = np.bincount(subway_assign_idx, minlength=len(adaptive_grid)).astype(int)\n",
        "\n",
        "adaptive_grid['crime_count_in_circle'] = crime_counts\n",
        "adaptive_grid['violent_crime_count_in_circle'] = violent_counts\n",
        "adaptive_grid['violent_crime_ratio_in_circle'] = np.where(\n",
        "    adaptive_grid['crime_count_in_circle'] > 0,\n",
        "    adaptive_grid['violent_crime_count_in_circle'] / adaptive_grid['crime_count_in_circle'],\n",
        "    0.0,\n",
        ")\n",
        "adaptive_grid['restaurant_count_in_circle'] = restaurant_counts\n",
        "adaptive_grid['subway_count_in_circle'] = subway_counts\n",
        "adaptive_grid['crime_assignment_median_dist_m'] = float(np.median(crime_assign_dist)) if len(crime_assign_dist) else 0.0\n",
        "adaptive_grid['restaurant_assignment_median_dist_m'] = float(np.median(rest_assign_dist)) if len(rest_assign_dist) else 0.0\n",
        "adaptive_grid['subway_assignment_median_dist_m'] = float(np.median(subway_assign_dist)) if len(subway_assign_dist) else 0.0\n",
        "\n",
        "adaptive_grid = adaptive_grid.sort_values('adaptive_cell_id').reset_index(drop=True)\n",
        "adaptive_output = OUTPUT_DIR / f'adaptive_sparse_grid_rentals_k{K_RENTALS_PER_CELL}.csv'\n",
        "adaptive_grid.to_csv(adaptive_output, index=False)\n",
        "\n",
        "adaptive_grid.head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b35c82",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:10.504405Z",
          "iopub.status.busy": "2026-02-24T22:58:10.504345Z",
          "iopub.status.idle": "2026-02-24T22:58:10.507607Z",
          "shell.execute_reply": "2026-02-24T22:58:10.507320Z"
        }
      },
      "outputs": [],
      "source": [
        "adaptive_summary = pd.DataFrame([\n",
        "    {'metric': 'rental_points_total', 'value': int(len(rent_core))},\n",
        "    {'metric': 'rentals_per_cell_target', 'value': K_RENTALS_PER_CELL},\n",
        "    {'metric': 'adaptive_cells_created', 'value': int(len(adaptive_grid))},\n",
        "    {'metric': 'retained_rental_points', 'value': int(adaptive_grid['rental_count'].sum())},\n",
        "    {'metric': 'leftover_rental_points', 'value': int(len(leftover_rental_indices))},\n",
        "    {'metric': 'min_radius_m', 'value': round(float(adaptive_grid['radius_m'].min()), 3)},\n",
        "    {'metric': 'median_radius_m', 'value': round(float(adaptive_grid['radius_m'].median()), 3)},\n",
        "    {'metric': 'max_radius_m', 'value': round(float(adaptive_grid['radius_m'].max()), 3)},\n",
        "])\n",
        "\n",
        "adaptive_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1038d590",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:10.508714Z",
          "iopub.status.busy": "2026-02-24T22:58:10.508660Z",
          "iopub.status.idle": "2026-02-24T22:58:10.559368Z",
          "shell.execute_reply": "2026-02-24T22:58:10.558981Z"
        }
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11, 6))\n",
        "plot_df = adaptive_grid.copy()\n",
        "\n",
        "# Radius is the key density signal: sparse areas -> larger circles.\n",
        "sns.scatterplot(\n",
        "    data=plot_df,\n",
        "    x='centroid_lon',\n",
        "    y='centroid_lat',\n",
        "    size='radius_m',\n",
        "    hue='radius_m',\n",
        "    palette='viridis',\n",
        "    sizes=(30, 500),\n",
        "    alpha=0.75,\n",
        "    legend=False,\n",
        ")\n",
        "\n",
        "plt.title(f'Adaptive Cells (k={K_RENTALS_PER_CELL} Rentals Each): Circle Size Encodes Local Density')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d60399bb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:10.562320Z",
          "iopub.status.busy": "2026-02-24T22:58:10.562240Z",
          "iopub.status.idle": "2026-02-24T22:58:11.284886Z",
          "shell.execute_reply": "2026-02-24T22:58:11.284437Z"
        }
      },
      "outputs": [],
      "source": [
        "map_sample = adaptive_grid.sample(min(4000, len(adaptive_grid)), random_state=42)\n",
        "fig = px.scatter_mapbox(\n",
        "    map_sample,\n",
        "    lat='centroid_lat',\n",
        "    lon='centroid_lon',\n",
        "    color='radius_m',\n",
        "    size='radius_m',\n",
        "    hover_data={\n",
        "        'adaptive_cell_id': True,\n",
        "        'rental_count': True,\n",
        "        'radius_m': ':.1f',\n",
        "        'crime_count_in_circle': True,\n",
        "        'restaurant_count_in_circle': True,\n",
        "        'subway_count_in_circle': True,\n",
        "    },\n",
        "    title=f'Adaptive Sparse Grid: Exactly {K_RENTALS_PER_CELL} Rentals Per Cell (Radius reflects density)',\n",
        "    color_continuous_scale='Turbo',\n",
        "    zoom=9.2,\n",
        "    height=580,\n",
        ")\n",
        "fig.update_layout(mapbox_style='carto-positron', margin={'l': 0, 'r': 0, 't': 40, 'b': 0})\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c6ea654",
      "metadata": {},
      "source": [
        "## Validation Checks\n",
        "\n",
        "**Question:** Did preprocessing and sparse-grid creation complete with integrity?\n",
        "\n",
        "**Method:** Assertions on coordinate validity, projected coordinates, point assignment completeness, and artifact existence.\n",
        "\n",
        "**What we found:** Checks below must pass for this notebook execution to be valid.\n",
        "\n",
        "**Why this matters for next-phase clustering:** It guarantees the grid foundation is technically sound before unsupervised modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75d8c53",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:11.301185Z",
          "iopub.status.busy": "2026-02-24T22:58:11.301089Z",
          "iopub.status.idle": "2026-02-24T22:58:11.309244Z",
          "shell.execute_reply": "2026-02-24T22:58:11.308884Z"
        }
      },
      "outputs": [],
      "source": [
        "# coordinate validity after cleaning\n",
        "assert valid_coord_mask(crime, 'Lat', 'Long').all()\n",
        "assert valid_coord_mask(rent, 'Lat', 'Long').all()\n",
        "assert valid_coord_mask(rest_venue, 'Restaurant Latitude', 'Restaurant Longitude').all()\n",
        "assert valid_coord_mask(subway, 'latitude', 'longitude').all()\n",
        "\n",
        "# projected coordinates must exist and be finite\n",
        "for df in [crime_proj, rent_proj, rest_proj, subway_proj]:\n",
        "    assert df[['x_m', 'y_m']].notna().all().all()\n",
        "    assert np.isfinite(df[['x_m', 'y_m']].to_numpy()).all()\n",
        "\n",
        "# adaptive-cell integrity\n",
        "assert len(adaptive_grid) > 0\n",
        "assert (adaptive_grid['rental_count'] == K_RENTALS_PER_CELL).all()\n",
        "assert adaptive_grid[['radius_m', 'cell_area_km2']].gt(0).all().all()\n",
        "\n",
        "# retained rental points consistency\n",
        "expected_retained = len(rent_core) - len(leftover_rental_indices)\n",
        "assert int(adaptive_grid['rental_count'].sum()) == int(expected_retained)\n",
        "\n",
        "\n",
        "# non-overlap assignment integrity (each point counted once)\n",
        "assert int(adaptive_grid['crime_count_in_circle'].sum()) == int(len(crime_proj))\n",
        "assert int(adaptive_grid['restaurant_count_in_circle'].sum()) == int(len(rest_proj))\n",
        "assert int(adaptive_grid['subway_count_in_circle'].sum()) == int(len(subway_proj))\n",
        "\n",
        "# outputs\n",
        "assert (OUTPUT_DIR / 'data_quality_audit.csv').exists()\n",
        "assert (OUTPUT_DIR / 'preprocessing_drop_log.csv').exists()\n",
        "assert adaptive_output.exists() and adaptive_output.stat().st_size > 0\n",
        "\n",
        "validation = pd.DataFrame([\n",
        "    {'check': 'rental_points_total', 'value': int(len(rent_core)), 'status': 'pass'},\n",
        "    {'check': 'adaptive_cells_created', 'value': int(len(adaptive_grid)), 'status': 'pass'},\n",
        "    {'check': 'rentals_per_cell_min', 'value': int(adaptive_grid['rental_count'].min()), 'status': 'pass'},\n",
        "    {'check': 'rentals_per_cell_max', 'value': int(adaptive_grid['rental_count'].max()), 'status': 'pass'},\n",
        "    {'check': 'leftover_rentals', 'value': int(len(leftover_rental_indices)), 'status': 'pass'},\n",
        "    {'check': 'crime_count_no_overlap', 'value': int(adaptive_grid['crime_count_in_circle'].sum()), 'status': 'pass'},\n",
        "    {'check': 'restaurant_count_no_overlap', 'value': int(adaptive_grid['restaurant_count_in_circle'].sum()), 'status': 'pass'},\n",
        "    {'check': 'subway_count_no_overlap', 'value': int(adaptive_grid['subway_count_in_circle'].sum()), 'status': 'pass'},\n",
        "])\n",
        "\n",
        "validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa8575a",
      "metadata": {},
      "source": [
        "## Phase 2 Handoff Notes\n",
        "\n",
        "This notebook intentionally stops at adaptive spatial-unit construction.\n",
        "\n",
        "Next phase should:\n",
        "1. aggregate richer engineered features per adaptive cell,\n",
        "2. normalize/transform features,\n",
        "3. run clustering,\n",
        "4. evaluate overlap/separation and spatial coherence.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f761a14a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-24T22:58:11.310342Z",
          "iopub.status.busy": "2026-02-24T22:58:11.310282Z",
          "iopub.status.idle": "2026-02-24T22:58:11.312473Z",
          "shell.execute_reply": "2026-02-24T22:58:11.312067Z"
        }
      },
      "outputs": [],
      "source": [
        "elapsed_minutes = (time.time() - NOTEBOOK_START) / 60\n",
        "print(f'Notebook runtime: {elapsed_minutes:.2f} minutes')\n",
        "print('Saved artifacts:')\n",
        "for p in sorted(OUTPUT_DIR.glob('*.csv')):\n",
        "    print(' -', p)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ce9122",
      "metadata": {},
      "source": [
        "## Step 3: Feature Engineering for Clustering\n",
        "\n",
        "This section creates clustering-ready features focused on neighbourhood profile insights (not location encoding):\n",
        "- Density and ratio features (per-km2 and amenity/safety balance)\n",
        "- One-hot encoding for tier columns (rent, safety, transit, food access)\n",
        "- Frequency encoding for tier combinations to capture how common each profile is\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d273b45",
      "metadata": {},
      "source": [
        "### Why These Features Are Relevant Here\n",
        "\n",
        "The goal is to understand neighbourhood profiles across rent, safety, transit, and food access tiers. So the encoding strategy is designed to make those tier patterns explicit and comparable.\n",
        "\n",
        "- Density features (per_km2) normalize counts across variable-size adaptive cells.\n",
        "- Ratio features expose trade-offs (for example amenities relative to crime) that raw counts can hide.\n",
        "- Log features reduce the influence of extreme count hotspots.\n",
        "- One-hot tier features make each tier directly interpretable in cluster profiles.\n",
        "- Combo-frequency features represent how common or rare each tier combination is, which helps surface emerging vs typical profiles.\n",
        "- Spatial/index IDs are kept for traceability only, not as encoding targets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25bde73",
      "metadata": {},
      "source": [
        "#### 1) Numeric Density, Ratio, and Log Features\n",
        "Why this matters:\n",
        "- crime_per_km2, restaurant_per_km2, subway_per_km2: comparable intensity across variable-size adaptive cells.\n",
        "- amenity_to_crime_ratio, subway_to_crime_ratio: how much convenience exists relative to safety pressure.\n",
        "- rent_to_amenity_ratio: affordability context against local offerings.\n",
        "- log1p features: compresses extreme tails common in urban count data while preserving ordering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6ba330",
      "metadata": {},
      "outputs": [],
      "source": [
        "fe = adaptive_grid.copy()\n",
        "eps = 1e-6\n",
        "\n",
        "fe['crime_per_km2'] = fe['crime_count_in_circle'] / (fe['cell_area_km2'] + eps)\n",
        "fe['violent_crime_per_km2'] = fe['violent_crime_count_in_circle'] / (fe['cell_area_km2'] + eps)\n",
        "fe['restaurant_per_km2'] = fe['restaurant_count_in_circle'] / (fe['cell_area_km2'] + eps)\n",
        "fe['subway_per_km2'] = fe['subway_count_in_circle'] / (fe['cell_area_km2'] + eps)\n",
        "\n",
        "fe['amenity_to_crime_ratio'] = fe['restaurant_count_in_circle'] / (fe['crime_count_in_circle'] + 1.0)\n",
        "fe['subway_to_crime_ratio'] = fe['subway_count_in_circle'] / (fe['crime_count_in_circle'] + 1.0)\n",
        "fe['rent_to_amenity_ratio'] = fe['mean_rent'] / (fe['restaurant_count_in_circle'] + 1.0)\n",
        "\n",
        "for col in ['crime_count_in_circle', 'violent_crime_count_in_circle', 'restaurant_count_in_circle', 'subway_count_in_circle', 'radius_m', 'mean_rent', 'mean_price_per_bedroom']:\n",
        "    fe[f'{col}_log1p'] = np.log1p(fe[col].clip(lower=0))\n",
        "\n",
        "fe[['crime_per_km2', 'restaurant_per_km2', 'amenity_to_crime_ratio']].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1934b131",
      "metadata": {},
      "source": [
        "#### 2) One-Hot Encoding for Low-Cardinality Tiers\n",
        "Why this matters:\n",
        "- rent_tier describes market level (lower to premium).\n",
        "- safety_tier turns violent-crime ratio into interpretable risk bands.\n",
        "- transit_tier and food_access_tier summarize access level instead of raw counts.\n",
        "- One-hot keeps categories model-ready and avoids imposing fake numeric distance between labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8f575ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# where rent_q1 is the lowest rent quartile and rent_q4 is the highest\n",
        "fe['rent_tier'] = pd.qcut(fe['mean_rent'], q=4, labels=['rent_q1', 'rent_q2', 'rent_q3', 'rent_q4'], duplicates='drop')\n",
        "\n",
        "# where safe <10% violent crime, moderate 10-25%, higher_risk >25%\n",
        "# -0.01 was used just to ensure that 0% violent crime cases are included in the 'safe' tier\n",
        "fe['safety_tier'] = pd.cut(fe['violent_crime_ratio_in_circle'], bins=[-0.01, 0.10, 0.25, 1.0], labels=['safe', 'moderate', 'higher_risk'])\n",
        "\n",
        "\n",
        "fe['transit_tier'] = pd.cut(fe['subway_count_in_circle'], bins=[-0.01, 0.5, 1.5, 3.5, np.inf], labels=['none', 'low', 'medium', 'high'])\n",
        "\n",
        "# where food_q1 is the lowest food access quartile and food_q4 is the highest\n",
        "fe['food_access_tier'] = pd.qcut(fe['restaurant_count_in_circle'], q=4, labels=['food_q1', 'food_q2', 'food_q3', 'food_q4'], duplicates='drop')\n",
        "\n",
        "low_card_cols = ['rent_tier', 'safety_tier', 'transit_tier', 'food_access_tier']\n",
        "for c in low_card_cols:\n",
        "    fe[c] = fe[c].astype('object').fillna('missing')\n",
        "\n",
        "one_hot = pd.get_dummies(fe[low_card_cols], prefix=low_card_cols, dtype=int)\n",
        "one_hot.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed6500ef",
      "metadata": {},
      "source": [
        "#### 3) Frequency Encoding for Tier Combinations\n",
        "Why this matters:\n",
        "- Individual tiers are informative, but combinations (for example high transit + moderate safety) often define the actual neighbourhood profile.\n",
        "- Frequency encoding on combinations captures how common each profile is across the city.\n",
        "- This keeps the encoding compact and aligned with our insight goal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa7414ea",
      "metadata": {},
      "outputs": [],
      "source": [
        "tier_cols = ['rent_tier', 'safety_tier', 'transit_tier', 'food_access_tier']\n",
        "\n",
        "combo_defs = {\n",
        "    'rent_safety_combo': ['rent_tier', 'safety_tier'],\n",
        "    'transit_food_combo': ['transit_tier', 'food_access_tier'],\n",
        "    'rent_transit_combo': ['rent_tier', 'transit_tier'],\n",
        "    'safety_transit_combo': ['safety_tier', 'transit_tier'],\n",
        "    'all_tiers_combo': tier_cols,\n",
        "}\n",
        "\n",
        "tier_combo_freq = pd.DataFrame(index=fe.index)\n",
        "for combo_name, cols in combo_defs.items():\n",
        "    combo_key = fe[cols].astype(str).agg('|'.join, axis=1)\n",
        "    combo_map = combo_key.value_counts(normalize=True)\n",
        "    tier_combo_freq[f'{combo_name}_freq'] = combo_key.map(combo_map).astype(float)\n",
        "\n",
        "fe = pd.concat([fe, tier_combo_freq], axis=1)\n",
        "fe[[c for c in fe.columns if c.endswith('_combo_freq')]].head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b97af1b0",
      "metadata": {},
      "source": [
        "#### 4) Build Final Feature Matrix (Pre-Scaling)\n",
        "Why this matters:\n",
        "- This step assembles engineered numeric features, one-hot tier columns, and combo-frequency columns in one model-ready table.\n",
        "- We remove identifier and coordinate fields used for joins/traceability so clustering focuses on behavioural profile features.\n",
        "- Scaling is done in the next dedicated section so the fitted scaler can be reused in the app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4893d56a",
      "metadata": {},
      "outputs": [],
      "source": [
        "drop_from_features = {\n",
        "    'adaptive_cell_id', 'member_rental_idx', 'seed_grid_x', 'seed_grid_y',\n",
        "    'spatial_key_1km', 'centroid_lon', 'centroid_lat', 'x_centroid', 'y_centroid'\n",
        "}\n",
        "\n",
        "numeric_candidates = [c for c in fe.columns if c not in drop_from_features and pd.api.types.is_numeric_dtype(fe[c])]\n",
        "numeric_features = fe[numeric_candidates].replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
        "\n",
        "feature_matrix = pd.concat([numeric_features, one_hot], axis=1)\n",
        "feature_matrix = feature_matrix.loc[:, ~feature_matrix.columns.duplicated()].copy()\n",
        "\n",
        "feature_output = OUTPUT_DIR / f'adaptive_feature_engineered_k{K_RENTALS_PER_CELL}.csv'\n",
        "feature_row_output = OUTPUT_DIR / f'adaptive_feature_rows_k{K_RENTALS_PER_CELL}.csv'\n",
        "\n",
        "feature_matrix.to_csv(feature_output, index=False)\n",
        "fe.to_csv(feature_row_output, index=False)\n",
        "\n",
        "print('Feature matrix prepared (not scaled yet).')\n",
        "print(f'Rows: {len(feature_matrix):,} | Features: {feature_matrix.shape[1]:,}')\n",
        "print('Saved:')\n",
        "print(' -', feature_output)\n",
        "print(' -', feature_row_output)\n",
        "feature_matrix.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10bf4904",
      "metadata": {},
      "source": [
        "#### 5) Scale Features with StandardScaler and Save Scaler for App\n",
        "Why StandardScaler specifically:\n",
        "- StandardScaler applies z-score normalization: each feature becomes (value - mean) / std.\n",
        "- This centers features around 0 and gives them unit variance, so rent, density, ratios, and encoded columns are on a comparable scale.\n",
        "\n",
        "Why scaling is important for this notebook:\n",
        "- Our next step is clustering, and common clustering methods (especially KMeans and distance-based approaches) use Euclidean distance.\n",
        "- Without scaling, large-magnitude variables (for example rent) dominate distance calculations and can hide safety/accessibility signals.\n",
        "- With scaling, clusters reflect multi-factor neighbourhood profiles instead of mainly whichever feature has the largest numeric range.\n",
        "\n",
        "Why saving the fitted scaler matters for the app:\n",
        "- The app must transform incoming data with the exact same means/stds used during analysis/training.\n",
        "- Re-fitting a new scaler in production would shift feature space and make cluster assignments inconsistent with notebook results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c2044a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "scaler = StandardScaler()\n",
        "feature_matrix_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(feature_matrix),\n",
        "    columns=feature_matrix.columns,\n",
        "    index=feature_matrix.index,\n",
        ")\n",
        "\n",
        "scaled_output = OUTPUT_DIR / f'adaptive_feature_matrix_scaled_k{K_RENTALS_PER_CELL}.csv'\n",
        "app_artifact_dir = OUTPUT_DIR.parent / 'app'\n",
        "app_artifact_dir.mkdir(parents=True, exist_ok=True)\n",
        "scaler_output = app_artifact_dir / f'adaptive_standard_scaler_k{K_RENTALS_PER_CELL}.joblib'\n",
        "\n",
        "feature_matrix_scaled.to_csv(scaled_output, index=False)\n",
        "joblib.dump(scaler, scaler_output)\n",
        "\n",
        "print('Scaling complete.')\n",
        "print('Saved:')\n",
        "print(' -', scaled_output)\n",
        "print(' -', scaler_output)\n",
        "feature_matrix_scaled.head(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc9d704",
      "metadata": {},
      "source": [
        "## Phase 3 Hand Off Notes\n",
        "This notebook intentionally stops after StandardScaler()\n",
        "\n",
        "The next sections should:\n",
        "1. Perform feature selection\n",
        "2. PCA\n",
        "3. K-Means + DBSCAN + HDBSCAN\n",
        "4. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29500fdb",
      "metadata": {},
      "source": [
        "## Step 4: Feature Selection (Cluster-Stability Permutation Importance)\n",
        "\n",
        "**Question:** Which features actually contribute to meaningful clustering structure, and which are noise?\n",
        "\n",
        "**Method:**\n",
        "- Since this is a **pure unsupervised** workflow (no target/label column), traditional supervised feature importance (e.g., Random Forest impurity) is not valid.\n",
        "- Instead, we use **cluster-stability permutation importance**:\n",
        "  1. Fit a baseline KMeans on the scaled feature matrix and compute the silhouette score.\n",
        "  2. For each feature, randomly permute its values (breaking its relationship with other features) and re-score.\n",
        "  3. Importance = mean drop in silhouette score when that feature is permuted.\n",
        "  4. Features that cause a large silhouette drop are important to cluster structure; those with zero or negative drop are noise.\n",
        "- Selection rule: **keep features until cumulative normalized importance reaches 90%** (configurable).\n",
        "\n",
        "**Why this matters for clustering:** Removing noisy features improves cluster separation, reduces the curse of dimensionality, and makes cluster profiles more interpretable.\n",
        "\n",
        "**Limitations of this approach (transparency):**\n",
        "- Importance is measured relative to a specific k (number of clusters) and may shift if k changes.\n",
        "- Correlated features share importance; dropping one may inflate the other's contribution.\n",
        "- Permutation breaks marginal but also conditional structure, so importance is approximate.\n",
        "- Results should be treated as a ranking heuristic, not ground truth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950158d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# ── Configuration (all tuneable) ──────────────────────────────────\n",
        "FEATURE_SEL_K = 5            # KMeans k for stability measurement\n",
        "N_PERMUTATIONS = 10          # repeats per feature (variance reduction)\n",
        "CUMULATIVE_IMPORTANCE_THRESHOLD = 0.90\n",
        "FEATURE_SEL_SEED = 42\n",
        "MIN_FEATURES_KEEP = 5        # safety floor\n",
        "\n",
        "REPORT_FIG_DIR = Path('../reports/figures')\n",
        "FEATURE_SEL_DIR = Path('../outputs/feature_selection')\n",
        "REPORT_FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FEATURE_SEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ── Inputs ────────────────────────────────────────────────────────\n",
        "X_scaled = feature_matrix_scaled.copy()\n",
        "feature_names = X_scaled.columns.tolist()\n",
        "\n",
        "print(f'Input feature matrix shape: {X_scaled.shape}')\n",
        "print(f'Total features: {len(feature_names)}')\n",
        "print(f'All features numeric: {all(pd.api.types.is_numeric_dtype(X_scaled[c]) for c in feature_names)}')\n",
        "print(f'Target/label column: None (unsupervised workflow)')\n",
        "\n",
        "\n",
        "def run_feature_selection(\n",
        "    X: pd.DataFrame,\n",
        "    k: int = FEATURE_SEL_K,\n",
        "    n_permutations: int = N_PERMUTATIONS,\n",
        "    cumulative_threshold: float = CUMULATIVE_IMPORTANCE_THRESHOLD,\n",
        "    min_keep: int = MIN_FEATURES_KEEP,\n",
        "    random_state: int = FEATURE_SEL_SEED,\n",
        ") -> tuple[pd.DataFrame, list[str]]:\n",
        "    \"\"\"Cluster-stability permutation importance for unsupervised feature selection.\n",
        "\n",
        "    Returns (importance_df, selected_feature_names).\n",
        "    \"\"\"\n",
        "    cols = X.columns.tolist()\n",
        "    vals = X.values.copy()\n",
        "\n",
        "    km = KMeans(n_clusters=k, n_init=10, random_state=random_state)\n",
        "    baseline_labels = km.fit_predict(vals)\n",
        "    baseline_sil = silhouette_score(vals, baseline_labels)\n",
        "    print(f'\\nBaseline KMeans (k={k}) silhouette: {baseline_sil:.4f}')\n",
        "\n",
        "    importances = {}\n",
        "    for feat_idx, feat_name in enumerate(cols):\n",
        "        drops = []\n",
        "        for perm_i in range(n_permutations):\n",
        "            X_perm = vals.copy()\n",
        "            rng = np.random.RandomState(random_state + perm_i)\n",
        "            X_perm[:, feat_idx] = rng.permutation(X_perm[:, feat_idx])\n",
        "            perm_labels = km.predict(X_perm)\n",
        "            perm_sil = silhouette_score(X_perm, perm_labels)\n",
        "            drops.append(baseline_sil - perm_sil)\n",
        "        importances[feat_name] = float(np.mean(drops))\n",
        "\n",
        "    imp_df = (\n",
        "        pd.DataFrame({'feature': list(importances.keys()),\n",
        "                       'importance': list(importances.values())})\n",
        "        .sort_values('importance', ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    total_positive = imp_df['importance'].clip(lower=0).sum()\n",
        "    imp_df['importance_normalized'] = imp_df['importance'].clip(lower=0) / max(total_positive, 1e-12)\n",
        "    imp_df['cumulative_importance'] = imp_df['importance_normalized'].cumsum()\n",
        "\n",
        "    mask = imp_df['cumulative_importance'].shift(1, fill_value=0.0) < cumulative_threshold\n",
        "    selected = imp_df.loc[mask, 'feature'].tolist()\n",
        "    if len(selected) < min_keep:\n",
        "        selected = imp_df['feature'].head(min_keep).tolist()\n",
        "\n",
        "    return imp_df, selected\n",
        "\n",
        "\n",
        "importance_df, selected_features = run_feature_selection(X_scaled)\n",
        "\n",
        "print(f'\\n── Selection summary ──')\n",
        "print(f'Selection rule: cumulative importance >= {CUMULATIVE_IMPORTANCE_THRESHOLD}')\n",
        "print(f'Features kept : {len(selected_features)} / {len(feature_names)}')\n",
        "print(f'Features dropped: {len(feature_names) - len(selected_features)}')\n",
        "print(f'\\nTop 10 features by importance:')\n",
        "importance_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd19781",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Slide-ready feature importance bar chart ──────────────────────\n",
        "\n",
        "TOP_K_DISPLAY = min(25, len(importance_df))\n",
        "plot_df = importance_df.head(TOP_K_DISPLAY).copy()\n",
        "is_selected = plot_df['feature'].isin(selected_features)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "colors = ['#2563eb' if sel else '#94a3b8' for sel in is_selected]\n",
        "\n",
        "bars = ax.barh(\n",
        "    y=np.arange(TOP_K_DISPLAY)[::-1],\n",
        "    width=plot_df['importance_normalized'].values,\n",
        "    color=colors,\n",
        "    edgecolor='white',\n",
        "    linewidth=0.5,\n",
        "    height=0.72,\n",
        ")\n",
        "\n",
        "ax.set_yticks(np.arange(TOP_K_DISPLAY)[::-1])\n",
        "ax.set_yticklabels(plot_df['feature'].values, fontsize=9)\n",
        "ax.set_xlabel('Normalized Importance (silhouette-drop)', fontsize=11)\n",
        "ax.set_title(\n",
        "    f'Cluster-Stability Feature Importance (Top {TOP_K_DISPLAY})\\n'\n",
        "    f'KMeans k={FEATURE_SEL_K} · {N_PERMUTATIONS} permutations · '\n",
        "    f'{len(selected_features)} features selected (≥{CUMULATIVE_IMPORTANCE_THRESHOLD:.0%} cumulative)',\n",
        "    fontsize=12, fontweight='bold', pad=14,\n",
        ")\n",
        "ax.axvline(0, color='grey', linewidth=0.5)\n",
        "\n",
        "from matplotlib.patches import Patch\n",
        "ax.legend(\n",
        "    handles=[Patch(facecolor='#2563eb', label='Selected'),\n",
        "             Patch(facecolor='#94a3b8', label='Dropped')],\n",
        "    loc='lower right', fontsize=9, framealpha=0.9,\n",
        ")\n",
        "\n",
        "sns.despine(left=True)\n",
        "plt.tight_layout()\n",
        "\n",
        "fig_path = REPORT_FIG_DIR / 'feature_importance.png'\n",
        "fig.savefig(fig_path, dpi=200, bbox_inches='tight')\n",
        "print(f'Saved: {fig_path}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a7918ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── Save artifacts and build filtered matrix ──────────────────────\n",
        "\n",
        "selected_features_path = FEATURE_SEL_DIR / 'selected_features.json'\n",
        "importance_csv_path = FEATURE_SEL_DIR / 'feature_importance.csv'\n",
        "\n",
        "with open(selected_features_path, 'w') as f:\n",
        "    json.dump({\n",
        "        'selected_features': selected_features,\n",
        "        'selection_method': 'cluster_stability_permutation_importance',\n",
        "        'params': {\n",
        "            'kmeans_k': FEATURE_SEL_K,\n",
        "            'n_permutations': N_PERMUTATIONS,\n",
        "            'cumulative_threshold': CUMULATIVE_IMPORTANCE_THRESHOLD,\n",
        "            'random_state': FEATURE_SEL_SEED,\n",
        "        },\n",
        "        'total_features_before': len(feature_names),\n",
        "        'total_features_after': len(selected_features),\n",
        "    }, f, indent=2)\n",
        "\n",
        "importance_df.to_csv(importance_csv_path, index=False)\n",
        "\n",
        "feature_matrix_selected = feature_matrix_scaled[selected_features].copy()\n",
        "\n",
        "selected_matrix_path = FEATURE_SEL_DIR / f'feature_matrix_selected_k{K_RENTALS_PER_CELL}.csv'\n",
        "feature_matrix_selected.to_csv(selected_matrix_path, index=False)\n",
        "\n",
        "print('── Artifacts saved ──')\n",
        "print(f'  Selected features list : {selected_features_path}')\n",
        "print(f'  Full importance table  : {importance_csv_path}')\n",
        "print(f'  Filtered feature matrix: {selected_matrix_path}')\n",
        "print(f'\\n── Before / After ──')\n",
        "print(f'  Before: {X_scaled.shape[0]} rows × {X_scaled.shape[1]} features')\n",
        "print(f'  After : {feature_matrix_selected.shape[0]} rows × {feature_matrix_selected.shape[1]} features')\n",
        "print(f'\\n── Reproducibility ──')\n",
        "print(f'  random_state = {FEATURE_SEL_SEED}')\n",
        "print(f'  KMeans n_init = 10')\n",
        "\n",
        "feature_matrix_selected.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1607f172",
      "metadata": {},
      "source": [
        "## Step 5: Dimensionality Reduction (PCA + UMAP)\n",
        "\n",
        "**Question:** Do the selected features reveal separable cluster structure in 2D?\n",
        "\n",
        "**Method:**\n",
        "1. **Standardize** the selected features (already scaled via StandardScaler in Step 3, reused here).\n",
        "2. **PCA** — linear projection that maximizes explained variance. We report the variance explained by each component and produce a 2D scatter.\n",
        "3. **UMAP** — non-linear manifold embedding that preserves local neighbourhood structure. Better at revealing non-convex cluster shapes.\n",
        "\n",
        "**Why PCA first:**\n",
        "- PCA is deterministic (no random initialization beyond sign flips), cheap, and tells us how much variance is concentrated in the first few axes.\n",
        "- If the first two PCs capture ≥50% variance, linear structure alone may be sufficient.\n",
        "\n",
        "**Why UMAP second:**\n",
        "- UMAP often reveals structure that PCA cannot (e.g., non-linear manifolds, nested clusters).\n",
        "- It is the standard pre-clustering visualization in modern ML workflows.\n",
        "\n",
        "**Plotting note:** No cluster labels exist yet (clustering is the next phase), so both plots are labelled \"pre-clustering embedding.\" If cluster labels were available they would be overlaid automatically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3046150a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# ── Configuration ─────────────────────────────────────────────────\n",
        "PCA_N_COMPONENTS_REPORT = min(10, feature_matrix_selected.shape[1])\n",
        "PCA_RANDOM_STATE = 42\n",
        "\n",
        "# ── Fit PCA ───────────────────────────────────────────────────────\n",
        "\n",
        "def run_pca(X: pd.DataFrame, n_components: int = PCA_N_COMPONENTS_REPORT,\n",
        "            random_state: int = PCA_RANDOM_STATE) -> tuple[PCA, np.ndarray]:\n",
        "    \"\"\"Fit PCA and return (fitted model, 2D coordinates).\"\"\"\n",
        "    pca = PCA(n_components=n_components, random_state=random_state)\n",
        "    coords = pca.fit_transform(X.values)\n",
        "    return pca, coords\n",
        "\n",
        "\n",
        "pca_model, pca_coords = run_pca(feature_matrix_selected)\n",
        "\n",
        "evr = pca_model.explained_variance_ratio_\n",
        "print('── PCA Explained Variance ──')\n",
        "for i, v in enumerate(evr):\n",
        "    cum = evr[:i+1].sum()\n",
        "    print(f'  PC{i+1:2d}: {v:.4f}  (cumulative: {cum:.4f})')\n",
        "\n",
        "print(f'\\nFirst 2 PCs explain {evr[:2].sum():.2%} of total variance.')\n",
        "\n",
        "# ── 2D Scatter ────────────────────────────────────────────────────\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "scatter = ax.scatter(\n",
        "    pca_coords[:, 0], pca_coords[:, 1],\n",
        "    c='#2563eb', alpha=0.55, s=28, edgecolors='white', linewidths=0.3,\n",
        ")\n",
        "ax.set_xlabel(f'PC1 ({evr[0]:.1%} variance)', fontsize=11)\n",
        "ax.set_ylabel(f'PC2 ({evr[1]:.1%} variance)', fontsize=11)\n",
        "ax.set_title(\n",
        "    f'PCA 2D Embedding — Pre-Clustering\\n'\n",
        "    f'{feature_matrix_selected.shape[1]} selected features · '\n",
        "    f'{feature_matrix_selected.shape[0]} spatial units · '\n",
        "    f'PC1+PC2 = {evr[:2].sum():.1%} variance',\n",
        "    fontsize=12, fontweight='bold', pad=14,\n",
        ")\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "\n",
        "pca_fig_path = REPORT_FIG_DIR / 'pca_2d.png'\n",
        "fig.savefig(pca_fig_path, dpi=200, bbox_inches='tight')\n",
        "print(f'\\nSaved: {pca_fig_path}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ff15c7f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from umap.umap_ import UMAP as UMAPReducer\n",
        "\n",
        "# ── Configuration (all tuneable) ──────────────────────────────────\n",
        "UMAP_N_NEIGHBORS = 15\n",
        "UMAP_MIN_DIST = 0.1\n",
        "UMAP_METRIC = 'euclidean'\n",
        "UMAP_RANDOM_STATE = 42\n",
        "\n",
        "\n",
        "def run_umap(X: pd.DataFrame, n_neighbors: int = UMAP_N_NEIGHBORS,\n",
        "             min_dist: float = UMAP_MIN_DIST, metric: str = UMAP_METRIC,\n",
        "             random_state: int = UMAP_RANDOM_STATE) -> np.ndarray:\n",
        "    \"\"\"Fit UMAP and return 2D coordinates.\"\"\"\n",
        "    reducer = UMAPReducer(\n",
        "        n_components=2,\n",
        "        n_neighbors=n_neighbors,\n",
        "        min_dist=min_dist,\n",
        "        metric=metric,\n",
        "        random_state=random_state,\n",
        "    )\n",
        "    return reducer.fit_transform(X.values)\n",
        "\n",
        "\n",
        "umap_coords = run_umap(feature_matrix_selected)\n",
        "\n",
        "# ── 2D Scatter ────────────────────────────────────────────────────\n",
        "fig, ax = plt.subplots(figsize=(9, 7))\n",
        "scatter = ax.scatter(\n",
        "    umap_coords[:, 0], umap_coords[:, 1],\n",
        "    c='#7c3aed', alpha=0.55, s=28, edgecolors='white', linewidths=0.3,\n",
        ")\n",
        "ax.set_xlabel('UMAP-1', fontsize=11)\n",
        "ax.set_ylabel('UMAP-2', fontsize=11)\n",
        "ax.set_title(\n",
        "    f'UMAP 2D Embedding — Pre-Clustering\\n'\n",
        "    f'{feature_matrix_selected.shape[1]} selected features · '\n",
        "    f'n_neighbors={UMAP_N_NEIGHBORS} · min_dist={UMAP_MIN_DIST} · '\n",
        "    f'metric={UMAP_METRIC}',\n",
        "    fontsize=12, fontweight='bold', pad=14,\n",
        ")\n",
        "sns.despine()\n",
        "plt.tight_layout()\n",
        "\n",
        "umap_fig_path = REPORT_FIG_DIR / 'umap_2d.png'\n",
        "fig.savefig(umap_fig_path, dpi=200, bbox_inches='tight')\n",
        "print(f'Saved: {umap_fig_path}')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8e5e7b",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('═' * 60)\n",
        "print('  DIMENSIONALITY REDUCTION — VALIDATION SUMMARY')\n",
        "print('═' * 60)\n",
        "\n",
        "print(f'\\n  Input matrix      : {feature_matrix_selected.shape[0]} rows × {feature_matrix_selected.shape[1]} features')\n",
        "print(f'  Features (from)   : {X_scaled.shape[1]} total → {feature_matrix_selected.shape[1]} selected')\n",
        "\n",
        "print(f'\\n  ── PCA ──')\n",
        "print(f'  Components fitted : {PCA_N_COMPONENTS_REPORT}')\n",
        "print(f'  PC1 variance      : {evr[0]:.2%}')\n",
        "print(f'  PC2 variance      : {evr[1]:.2%}')\n",
        "print(f'  PC1+PC2 cumulative: {evr[:2].sum():.2%}')\n",
        "print(f'  Figure saved      : {pca_fig_path}')\n",
        "\n",
        "print(f'\\n  ── UMAP ──')\n",
        "print(f'  n_neighbors       : {UMAP_N_NEIGHBORS}')\n",
        "print(f'  min_dist          : {UMAP_MIN_DIST}')\n",
        "print(f'  metric            : {UMAP_METRIC}')\n",
        "print(f'  random_state      : {UMAP_RANDOM_STATE}')\n",
        "print(f'  Figure saved      : {umap_fig_path}')\n",
        "\n",
        "print(f'\\n  ── Reproducibility ──')\n",
        "print(f'  PCA random_state  : {PCA_RANDOM_STATE}')\n",
        "print(f'  UMAP random_state : {UMAP_RANDOM_STATE}')\n",
        "\n",
        "print(f'\\n  ── Files changed/added ──')\n",
        "print(f'  Modified: notebooks/toronto_housing_clustering_baseline.ipynb')\n",
        "print(f'  Added   : reports/figures/pca_2d.png')\n",
        "print(f'  Added   : reports/figures/umap_2d.png')\n",
        "print('═' * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (datasci)",
      "language": "python",
      "name": "datasci"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
